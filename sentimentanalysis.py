# -*- coding: utf-8 -*-
"""SentimentAnalysis_(1).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19zf8v2zTdG9KGMTc1piFyr-X0wRk1wWe

**Sentiment Analysis**

Here i'll analyze `Tweets` and build a model to predict whether the Tweet is positive, negative or neutral.

# Importing required libraries
"""

!pip install keras-preprocessing  # Install the missing package

import zipfile
import numpy as np
import pandas as pd
from io import  BytesIO
from keras import Sequential
from sklearn.svm import SVC
import matplotlib.pyplot as plt
from keras.utils import pad_sequences
from keras.utils import pad_sequences
from keras.utils import to_categorical
from sklearn.metrics import accuracy_score
from sklearn.naive_bayes import GaussianNB
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from keras_preprocessing.text import Tokenizer
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.callbacks import EarlyStopping
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from keras.layers import Dense, SimpleRNN, Embedding, Flatten, Input,LSTM,GRU,Dropout

"""# Data Collection"""

!mkdir -p ~/.kaggle # Create a .kaggle directory
!cp kaggle.json ~/.kaggle/ # Copy the kaggle.json file into the .kaggle directory
!chmod 600 ~/.kaggle/kaggle.json # Set appropriate permissions on the kaggle.json file to secure credentials

!kaggle datasets download -d abhi8923shriv/sentiment-analysis-dataset # Download the Sentiment Analysis dataset from Kaggle

# Create a ZipFile object to open the ZIP file located at '/content/dogs-vs-cats.zip' in read mode ('r')
zip_ref = zipfile.ZipFile('/content/sentiment-analysis-dataset.zip', 'r')

# Extract all contents of the ZIP file into the specified directory ('/content')
zip_ref.extractall('/content')

# Close the ZipFile object to free up resources
zip_ref.close()

# Load training and validation datasets from CSV files with 'latin1' encoding

train_ds = pd.read_csv('/content/train.csv', encoding='latin1')
validation_ds = pd.read_csv('/content/test.csv', encoding='latin1')

"""# Data Cleaning and Preprocessing

**Explore the datasets**
"""

# Take two columns from dataset

train_ds = train_ds[['text','sentiment']]
validation_ds = validation_ds[['text','sentiment']]

# print 5 sample data

validation_ds.sample(5)

# Drop missing values

train_ds = train_ds.dropna()
validation_ds = validation_ds.dropna()

# print shape of the training and validation dataset

print('Shape of the training data:',train_ds.shape)
print('Shape of the validation data:',validation_ds.shape)

"""# ploting"""

# Plot the bar plot
train_ds['sentiment'].value_counts().plot.bar(color='skyblue', edgecolor='black')

# Add title and labels
plt.title('Sentiment Distribution Count')
plt.xlabel('Sentiment')
plt.ylabel('Count')

# Show the plot
plt.show()

fig = plt.figure(figsize=(7,7))

colors = ('skyblue', 'lightcoral', 'green')

wp = {'linewidth':1, "edgecolor":'black'}

tags = train_ds['sentiment'].value_counts()/train_ds.shape[0]

# Calculate the number of sentiment categories
num_categories = len(tags)

# Create an explode tuple with the correct length
explode = (0.1,) * num_categories

tags.plot(kind='pie', autopct="%1.1f%%", shadow=True, colors=colors, startangle=90, wedgeprops=wp, explode=explode, label='Percentage rating')

graph = BytesIO()

fig.savefig(graph, format="png")

"""`40.5%` -- Neutral.

`31.2%` -- Positive.

`28.3%` -- Negative.

# Feature Engineering
"""

# Define a function to convert sentiment labels to numeric values
def function(sentiment):
    # If the sentiment is 'positive', return 0
    if sentiment == 'positive':
        return 0
    # If the sentiment is 'negative', return 1
    elif sentiment == 'negative':
        return 1
    # For any other sentiment, return 2
    else:
        return 2

# Apply the function to the 'sentiment' column of the train dataset
train_ds['sentiment'] = train_ds['sentiment'].apply(function)

# Apply the same function to the 'sentiment' column of the validation dataset
validation_ds['sentiment'] = validation_ds['sentiment'].apply(function)

# Convert the 'text' column from the dataset into a numpy array

x_train = np.array(train_ds['text'].tolist())
y_train = np.array(train_ds['sentiment'].tolist())
x_test = np.array(validation_ds['text'].tolist())
y_test = np.array(validation_ds['sentiment'].tolist())

# Print X_train

x_train

# Print y_test

y_test

# Convert the training and testing labels to one-hot encoded format

y_train = to_categorical(y_train, 3)
y_test = to_categorical(y_test, 3)

# Initialize the Tokenizer with a maximum number of words
tokenizer = Tokenizer(num_words=2000)

# Fit the tokenizer on the training data
tokenizer.fit_on_texts(x_train)

len(tokenizer.word_index)

# Convert text to sequences of integers

x_train = tokenizer.texts_to_sequences(x_train)
x_test = tokenizer.texts_to_sequences(x_test)

# Pad the sequences

x_train = pad_sequences(x_train, padding='post', maxlen=30)
x_test = pad_sequences(x_test, padding='post', maxlen=30)

x_train[49]

x_train.shape

"""# Model Building

"I'll begin with `Machine Learning` models.

**Machine Learning Models**
"""

# Convert one-hot encoded labels to single integer labels
y_train_int = np.argmax(y_train, axis=1)
y_test_int = np.argmax(y_test, axis=1)

models = {
    'Logistic Regression': LogisticRegression(multi_class='multinomial', solver='saga', max_iter=1000),
    'Decision Tree': DecisionTreeClassifier(),
    'Random Forest': RandomForestClassifier(),
    'Gradient Boosting': GradientBoostingClassifier(),
    'Support Vector Classifier': SVC(),
    'K-Nearest Neighbors': KNeighborsClassifier(),
    'Naive Bayes': GaussianNB()
}

# Dictionary to store accuracy results
results = {}

# Train and evaluate each model
for name, model in models.items():
    # Train the model
    model.fit(x_train, y_train_int)

    # Predict on training data
    y_train_pred = model.predict(x_train)
    train_accuracy = accuracy_score(y_train_int, y_train_pred)

    # Predict on test data
    y_test_pred = model.predict(x_test)
    test_accuracy = accuracy_score(y_test_int, y_test_pred)

    # Store the results
    results[name] = {
        'Training Accuracy': train_accuracy,
        'Test Accuracy': test_accuracy
    }

# Display the results
for name, accuracy in results.items():
    print(f"{name}:")
    print(f" - Training Accuracy: {accuracy['Training Accuracy']:.4f}")
    print(f" - Test Accuracy: {accuracy['Test Accuracy']:.4f}")
    print()

"""`I observed poor accuracy and signs of overfitting, so I'll proceed with Deep RNN.`

#### Deep RNN
"""

# Define and build a Sequential model with embedding, GRU layers, dropout, and a dense output layer for multi-class classification.


model = Sequential()
model.add(Embedding(input_dim=20000, output_dim=5, input_length=30))
model.add(GRU(100, return_sequences=True))
Dropout(0.5)
model.add(GRU(50))
model.add(Flatten())
model.add(Dense(3, activation='sigmoid'))
model.build(input_shape=(None, 30))

# Display a summary of the model's architecture and parameters

model.summary()

# Compile the model with Adam optimizer and categorical crossentropy loss for multi-class classification

model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Stop training when no improvement, restoring best weights

early_stopping = EarlyStopping(monitor='val_accuracy', patience=3, restore_best_weights=True)

# Train the model for 50 epochs with validation and early stopping

history = model.fit(x_train, y_train, epochs=50, validation_data=(x_test, y_test), callbacks=[early_stopping])

"""#### Plotting graphs for traning and validation score"""

# Create a figure with 1 row and 2 columns of subplots
plt.figure(figsize=(10, 5))

# Plot accuracy
plt.subplot(1, 2, 1)  # (rows, columns, index)
plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.title('Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()

# Plot loss
plt.subplot(1, 2, 2)  # (rows, columns, index)
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()

# Show the plots
plt.tight_layout()
plt.show()

"""#### Make Predictions"""

# Example texts
text1 = "The movie was bad bad bad, I will not recommend this movie to anyone"
text2 = "This is my day, I hope this will be a great day for me."
text3 = 'The price of one kg potato is 100BDT.'

# Tokenize and pad sequences
new_text_seq = tokenizer.texts_to_sequences([text1, text2, text3])  # Tokenize new texts
new_text_padded = pad_sequences(new_text_seq, padding='post', maxlen=35)  # Use max_len determined during training

# Make predictions
predictions = model.predict(new_text_padded)

# Get the class with the highest probability
predicted_class_index = predictions.argmax(axis=-1)

# Map indices to sentiment labels
sentiment_labels = {0: "Positive Sentiment", 1: "Negative Sentiment", 2: "Neutral Sentiment"}

# Print predictions
for i, text in enumerate([text1, text2, text3]):
    print(f"Text: '{text}'")
    print(f"Predicted Sentiment: {sentiment_labels[predicted_class_index[i]]}")

"""**Predictions:**

Text1: *'The movie was bad bad bad, I will not recommend this movie to anyone'*

Predicted Sentiment: **`Negative Sentiment`**

Text2: *'This is my day, I hope this will be a great day for me.'*

Predicted Sentiment: **`Positive Sentiment`**

Text2: *'The price of one kg potato is 100BDT.'*

Predicted Sentiment: **`Neutral Sentiment`**
"""



import pickle

# Assuming 'model' is your trained model
with open('model.pkl', 'wb') as file:
    pickle.dump(model, file)

# Load the model from the pickle file
with open('model.pkl', 'rb') as file:
    loaded_model = pickle.load(file)

# Save the tokenizer as a pickle file
with open('tokenizer.pkl', 'wb') as file:
    pickle.dump(tokenizer, file)

